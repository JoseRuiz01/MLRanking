# ğŸ§ª **Listwise Learning to Rank (LTR) for Lab Test Ranking**

Listwise Learning to Rank (LTR) optimizes the **entire ranking order** for a given queryâ€”unlike Pointwise or Pairwise approaches. It's especially effective for ranking **lab tests** by relevance to queries like:

> *"glucose in blood"*, *"bilirubin in plasma"*, *"white blood cells count"*

---

## ğŸ”§ **Step 1: Define the Listwise LTR Model**

Listwise LTR models learn a *ranking function* that optimizes evaluation metrics such as **NDCG** (*Normalized Discounted Cumulative Gain*).

### âš™ï¸ Workflow:

1. **Input**: A list of lab tests (documents) for a given query.  
2. **Scoring Function**: A model predicts a *relevance score* per test.  
3. **Loss Function**:
   - **eXtreme NDCG** â€“ a direct optimization of NDCG.
   - **LambdaRank** â€“ also NDCG-focused.
4. **Output**: A ranked list of lab tests based on predicted relevance.

---

## ğŸ§¹ **Step 2: Data Preparation**

We calculate *relevance scores* for lab tests by computen two different scoring procedures.


### 1. Traditional Scoring

#### ğŸ” 1.1 Define Query Features
- **Component**: Substance measured (e.g., *Glucose*)
- **System**: Environment of measurement (e.g., *Blood*, *Serum/Plasma*)

#### ğŸ§¼ 1.2. Preprocess Dataset
Each lab test includes:
- **Component**
- **System**

#### ğŸ¯ 1.3. Match Criteria
- **Exact Match**: Full match with the query term.
- **Partial Match**: Synonyms or semantically similar terms.

#### ğŸ§® 1.4. Scoring Scheme
- **Exact Match** (Component) = weight(component) * weight(component)
- **Partial Match** (Component) = weight(component)/2 * weight(component)
- **Exact Match** (System) = weight(system) * weight(system)
- **Partial Match** (System) = weight(system)/2 * weight(system) No Match = 0

### âš–ï¸ 3. Normalize Scores
Normalize scores between 0 and 1 using:
- **Normalized Score** = score / max_score


### ğŸ’¾ 4. Export Data
Save the processed data and scores into a new **CSV** file for model training.

---

## ğŸ› ï¸ **Step 3: Implement the Listwise LTR Model**

We use **LightGBM** due to its speed, simplicity, and support for listwise ranking.

### ğŸ“ 1. Dataset Preparation
- Load data from CSV.
- Encode categorical columns: `Query`, `Name`, `Component`, `System`, `Property`, `Measurement`.
- Create `Score_label` from `Normalized_Score`.
- Split into **train** and **test** sets.

### ğŸ“Š 2. LightGBM Dataset Setup
- **Features**: Encoded columns.
- **Grouping**: Group by `Query` (listwise requirement).
- **Labels**: Use `Score_label`.

### ğŸ§  3. Train the Model
- **Objective**: `rank_xendcg`
- **Approach**: Simulate *AdaRank*-style boosting and reweighting using LightGBM parameters.

### ğŸ“ˆ 4. Prediction
- Predict and normalize scores.
- Sort by `Query` and `Predicted Score`.
- Save results to `results.csv`.

---

## ğŸš€ **Step 4: Enhancing the Dataset**

To improve **NDCG**, we introduced new **features**, expanded **queries**, and added more **data**.

### ğŸ” 1. Expanded Queries
Added queries beyond the original three:
- `calcium in serum`
- `cells in urine`  
...including query variations like `calcium`, `urine`, `cells`, etc.

### ğŸ“¦ 2. Dataset Expansion
We queried **LOINC Search** for additional documents:
- bilirubin in plasma / bilirubin  
- calcium in serum / calcium  
- glucose in blood / glucose  
- leukocytes / white blood cells count  
- blood / urine / cells  

Saved results as CSVs.

---

## ğŸ“Š **Step 5: Model Evaluation**

We use multiple **metrics** to assess model performance:

| Metric           | Description                                           | Ideal Value |
|------------------|-------------------------------------------------------|-------------|
| **MSE**          | Mean Squared Error â€“ lower is better                  | 0           |
| **RÂ²**           | R-squared â€“ explains variance, higher is better       | 1           |
| **Spearman's Ï** | Rank correlation â€“ higher shows stronger ranking match| 1           |
| **NDCG**         | Normalized DCG â€“ higher is better ranking quality     | 1           |

---

### ğŸ“‰ **Dataset Performance Comparison**

| Dataset           | MSE    | RÂ²      | Spearman Ï | NDCG   | Notes                          |
|-------------------|--------|---------|------------|--------|---------------------------------|
| **Basic**         | 0.1642 | -2.5187 | 0.7265     | 0.9086 | Initial 3 queries               |
| **First Enhanced**| 0.0479 | -1.9010 | 0.4700     | 0.8533 | Added `calcium in serum`        |
| **Second Enhanced**| 0.0461| -0.8984 | 0.6024     | 0.9421 | Added `bilirubin`, `glucose`, `leukocytes` |
| **Third Enhanced**| 0.0252 | -0.4765 | 0.4983     | 0.9398 | Added `blood`, `serum or plasma`|
| **Fourth Enhanced**| 0.0450| -1.4383 | 0.4323     | 0.9448 | Added `cells in urine`          |
| **Fifth Enhanced** | 0.0191| -0.6009 | 0.4615     | **0.9517** | Final version with `cells`, `urine` |

---

### ğŸ“Œ **Per-Query NDCG (Fifth Dataset)**
- bilirubin in plasma: 0.9499
- calcium in serum: 0.9637
- cells in urine: 0.9448
- glucose in blood: 0.9663
- white blood cells count: 0.9339
